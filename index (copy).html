<!DOCTYPE HTML>
<!--
	Read Only by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Hamidreza Kasaei</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="icon" href="images/avatar.ico">
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

	<!-- Header -->
	<section id="header">
		<header>
			<span class="image avatar"><img src="images/hamid_new_photo.jpg" alt="" /></span>
			<h1 id="logo"><a href="#">Hamidreza Kasaei</a></h1>
			<p>Assistant Professor,<br />
			Department of Artificial Intelligence,<br />
			University of Groningen,  Netherlands.</p>
		</header>
		<nav id="nav">
			<ul>
				<li><a href="#about" class="active">About Me</a></li>
				<li><a href="#news">Latest News</a></li>
				<li><a href="#research">Research & Publication</a></li>
				<li><a href="#students">Students </a></li>
				<li><a href="#teaching">Teaching</a></li>
				<li><a href="#positions">Open Positions & Contact</a></li>
			</ul>
		</nav>
		<footer>
			<ul class="icons">
				<li><a target="_blank" href="https://twitter.com/HamidrezaKasaei" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
				<li><a target="_blank" href="https://scholar.google.com/citations?user=VFr_XuYAAAAJ&hl=en" class="icon fa-book"><span class="label">Google Scholar</span></a></li>
				<li><a target="_blank" href="https://www.linkedin.com/in/hamidreza-kasaei-49b83b57/" class="icon fa-linkedin"><span class="label">linkedin</span></a></li>
				<li><a target="_blank" href="https://github.com/SeyedHamidreza/" class="icon fa-github"><span class="label">Github</span></a></li>
				<li><a target="_blank" href="mailto:hamidreza.kasaei@rug.nl" class="icon fa-envelope"><span class="label">Email</span></a></li>
			</ul>
		</footer>
	</section>

	<!-- Wrapper -->
	<div id="wrapper">

		<!-- Main -->
		<div id="main">

			<!-- One -->
			<section id="about">
				<div>
				</div>
				<!-- <div class="image main" data-position="center">
				<img src="images/banner.jpg" alt="" />
				</div>-->
				<div class="container">

				<header class="major">
					<h2 style="text-align: center;">Hamidreza Kasaei
					<!-- <p style="text-align: center; font-size:9pt; color: rgb(0, 0, 0);">Department of Artificial Intelligence, University of Groningen, Netherlands.</p> -->
					</h2>

				</header>
				
				<div class="features">
					<article>
					<span class="image"><img src="images/hamid_new_photo.jpg" alt=""></span>
					<div class="inner">
						<p style="text-align: justify; color: rgb(0, 0, 0);">
							I am an Assistant Professor in the Department of Artificial Intelligence at the <b> <a href="http://rug.nl/" target="_blank" style="color: rgb(0,0, 255);"> University of Groningen</a></b>, the Netherlands. 
							My research group focuses on <b> <a href="https://www.ai.rug.nl/irl-lab/" target="_blank" style="color: rgb(0,0, 255);">Lifelong Interactive Robot Learning (IRL-Lab)</a></b>, which we work at the cutting edge of robotics research.						<!-- I joined the Department of Artificial Intelligence of the <b> <a href="http://rug.nl/" target="_blank" style="color: rgb(0,0, 255);"> University of Groningen</a></b>, the Netherlands, in 2018.  -->
						<!-- I have extensive background in computer vision, machine learning and robotics.  -->
						My main research interests lie in the area of <b  style="color: rgb(0, 0, 255);"> 
						3D Object Perception, Grasp Affordance, and Object Manipulation</b>. 
						<!-- Currently, I am developing an artificial cognitive system for robots to provide a tight coupling between perception and manipulation.  -->
						My goal is to achieve a breakthrough by enabling robots to incrementally learn from past experiences and safely
						interact with non-expert human users using data-efficient open-ended machine-learning techniques.


						<!-- This coupling is necessary for assistive robots, not only to perform manipulation tasks appropriately but also to robustly adapt to new environments by handling new objects.  -->
						</p>
					</div>
					</article>
				</div>
				
				<!-- <p style="text-align: justify; color: rgb(0, 0, 0);"> -->
					
					<!-- During my Ph.D., I got an opportunity to work on an FP7 Project named <b> 
					<a href="http://project-race.eu/" target="_blank" style="color: rgb(0,0, 255);"> RACE: Robustness by Autonomous Competence Enhancement</a></b>. 
					In this project, I was mainly responsible to develop interactive open-ended learning approaches to recognize multiple objects and their grasp affordances 
					concurrently.  During my master, I studied face recognition using single normal reference image and statistical features. Besides, I worked on middle size 
					soccer robot and humanoid robot and obtained different ranks in RoboCup competitions.  -->

						<!-- As researcher in Robotics, I envision a world where robots can easily assist, help, and empower people in a large diversity of scenarios and tasks. 
						To make this dream come true, I spend my days working on tools that allow robots to interact with humans smartly, naturally and safely. In this context, my research lies in the intersection of machine learning, robot control and human-robot interaction. -->

						<!-- Navigate my web page if you want to know more about me and my work. Enjoy! -->
				<!-- </p> -->
							
				<p style="text-align: justify; color: rgb(0, 0, 0);"></p>
				<div style="text-align: center; color: rgb(20, 0, 0);"> 
					/
					<a target="_blank" href="https://www.dropbox.com/s/sdmwirz9oeajqwy/CV_Hamidreza_Kasaei.pdf?dl=1"> CV</a> /
					<!-- <a target="_blank" href="documents/Hamidreza_Kasaei_PhD_thesis.pdf"> Bio</a> / -->
					<a target="_blank" href="documents/Hamidreza_Kasaei_PhD_thesis.pdf"> PhD Thesis</a> /
					<a href="#research">Publications</a> /
					<a target="_blank" href="https://scholar.google.com/citations?user=VFr_XuYAAAAJ&hl=en">Google Scholar</a> /
					<a target="_blank" href="https://www.researchgate.net/profile/Hamidreza_Kasaei?ev=hdr_xprf" target="_blank">ResearchGate </a> /
					<a target="_blank" href="https://www.linkedin.com/in/hamidreza-kasaei-49b83b57/">LinkedIn </a> / 
					<a target="_blank" href="https://github.com/SeyedHamidreza/">Github </a> / 
					<a target="_blank" href="https://www.youtube.com/channel/UCEPBEyQfiv1P8wfuYjuWy4Q">YouTube </a> / 
					<!-- <a target="_blank" href="mailto:hamidreza.kasaei@rug.nl">Email</a> /  -->
				</div>
			</p>
	</div>
</section>

<section id="news">
	<div class="container">
		
		<table>
			
			<tr>
				<td><a class="image"><img src="videos/good.gif" alt="" width="150%"/></a> </td>
				<td><a class="image"><img src="videos/serve_a_coke.gif" alt="" width="100%" /></a> </td>
				<td><a class="image"><img src="videos/neural_motion_planning.gif" alt="" width="100%" /></a> </td>
			</tr>

		</table>

	</div>
</section>

<!-- Two -->
<section id="news">
	<div class="container">
		<h3 style="color: rgb(0, 0, 0);" >Latest News</h3>

		<p style="text-align: justify; color: rgb(0, 0, 0);">


			<br /><b style="color: rgb(0, 150, 25);">* May. 2021:</b> I gave an invited talk on <b><a rel="nofollow" style="color: rgb(0,0, 255);"> Lifelong Robot Learning in Human-centric Environments: From Object Perception to Object Manipulation</a></b> at the <a href="https://www.lincoln.edu/" class="external text" title="https://www.lincoln.edu/" target="_blank" rel="nofollow" style="color: rgb(0,0, 255);" > University of Lincoln, UK </a>

			<br /><b style="color: rgb(0, 150, 25);">* May. 2021:</b> Our paper titled <b>Open-Ended Fine-Grained 3D Object Categorization by Combining Shape and Texture Features in Multiple Colorspaces</b> got accepted to <b><a 
				href="https://humanoids-2020.org/" target="_blank"  rel="nofollow" style="color: rgb(0,0, 255);">  IEEE-RAS International Conference on Humanoid Robots (Humanoids2020)! </b></a> Congrats Nils!
			</b>

			<br /><b style="color: rgb(0, 150, 25);">* April. 2021:</b> Our paper titled <b>3D_DEN: Open-ended 3D Object Recognition Using Dynamically Expandable Networks</b> got accepted to <b><a 
				href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7274989" target="_blank"  rel="nofollow" style="color: rgb(0,0, 255);"> IEEE Transactions on Cognitive and Developmental Systems! </b></a> Congrats Sudhakaran!
			</b>
			<br /><b style="color: rgb(0, 150, 25);">* March. 2021:</b> Our paper titled <b>Self-Imitation Learning by Planning</b> got accepted to <b><a 
				href="http://www.icra2021.org/" target="_blank"  rel="nofollow" style="color: rgb(0,0, 255);">ICRA2021! </b></a> Congrats Sha!
			</b>
			<br /><b style="color: rgb(0, 150, 25);">* Feb. 2021:</b> I am serving as  <b>associate editor </b>for the <b><a 
				href="https://www.iros2021.org/" class="external text" title="https://www.iros2021.org/" target="_blank"  rel="nofollow" style="color: rgb(0,0, 255);">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2021)
			</a></b>

			<br /><b style="color: rgb(0, 150, 25);">* Feb. 2021:</b> I gave an invited talk at the <a href="https://www.bosch-ai.com/" class="external text" title="https://www.bosch-ai.com/" target="_blank" rel="nofollow" style="color: rgb(0,0, 255);" > Bosch Center for Artificial Intelligence (BCAI) </a> on <b><a rel="nofollow" style="color: rgb(0,0, 255);"> Robots Beyond the Factory: Open-ended Robot Learning in Human-Centric Environments!</a></b>

			<br /><b style="color: rgb(0, 150, 25);">* Jan. 2021:</b> Our paper titled <b>Investigating the Importance of Shape Features, Color Constancy, Color Spaces and Similarity Measures in Open-Ended 3D Object Recognition</b> got accepted to <b><a 
				href="https://www.springer.com/journal/11370" class="external text" title="https://www.springer.com/journal/11370" target="_blank"  rel="nofollow" style="color: rgb(0,0, 255);">Intelligent Service Robotics Journal!
			</a></b>

			<br /><b style="color: rgb(0, 150, 25);">* Dec. 2020:</b> I am beyond excited to announce that I have accepted <b>a tenure track Assistant Professor position in the Department of Artificial Intelligence </b> at the <b><a 
			href="https://www.rug.nl/?lang=en" class="external text" title="https://www.rug.nl/?lang=en" target="_blank"  rel="nofollow" style="color: rgb(0,0, 255);"> University of Groningen, Netherlands!</a></b>

			<!-- <br /><b style="color: rgb(0, 150, 25);">* Nov. 2020:</b> Our paper titled <b>OrthographicNet: A Deep Transfer Learning Approach for 3D Object Recognition in Open-Ended Domains</b> got accepted to <b><a 
				href="http://www.ieee-asme-mechatronics.info/" class="external text" title="http://www.ieee-asme-mechatronics.info/" target="_blank"  rel="nofollow" style="color: rgb(0,0, 255);">IEEE Transactions on Mechatronics!
			</a></b> (IF 5.71).

			<br /><b style="color: rgb(0, 150, 25);">* October 2020:</b> I am serving as  <b>associate editor </b>for the <b><a 
				href="http://www.icra2021.org/" class="external text" title="http://www.icra2021.org/" target="_blank"  rel="nofollow" style="color: rgb(0,0, 255);">IEEE/RSJ International Conference on Robotics and Automation (ICRA 2021)
			</a></b>.

			<br /><b style="color: rgb(0, 150, 25);">* June 2020:</b> Our paper titled <b>Learning to Grasp 3D Objects using Deep Residual U-Nets</b> got accepted to <b><a 
				href="http://ro-man2020.unina.it/" class="external text" title="http://ro-man2020.unina.it/" target="_blank"  rel="nofollow" style="color: rgb(0,0, 255);">IEEE RO-MAN 2020!
			</a></b> Congrats Yikun!.
			


			<br /><b style="color: rgb(0, 150, 25);">* March 2020:</b> Our paper titled <b>Accelerating Reinforcement Learning for Reaching using Continuous Curriculum Learning</b> got accepted to 
			<b style="color: rgb(0,0, 255);">IJCNN 2020!</b> Congrats Sha!


			<br /><b style="color: rgb(0, 150, 25);">* October 2019:</b> I am serving as  <b>associate editor </b>for the <b><a 
				href="https://www.icra2020.org/" class="external text" title="https://www.icra2020.org/" target="_blank"  rel="nofollow" style="color: rgb(0,0, 255);">IEEE/RSJ International Conference on Robotics and Automation (ICRA 2020)
			</a></b>.

			<br /><b style="color: rgb(0, 150, 25);">* September 2019:</b> I will be teaching a new course on <b><a 
				href="https://rugcognitiverobotics.github.io/" class="external text" title="https://rugcognitiverobotics.github.io/" target="_blank"  rel="nofollow" style="color: rgb(0,0, 255);"> Cognitive Robotics
			</a></b>.

			<br /><b style="color: rgb(0, 150, 25);">* June 2019:</b> Our paper <b>Local-LDA: Open-Ended Learning of Latent Topics for 3D Object Recognition</b> got accepted at <b><a 
				href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" class="external text" title="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" target="_blank"  rel="nofollow" style="color: rgb(0,0, 255);">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI - IF = 17.730)</a></b>. 

			<br /><b style="color: rgb(0, 150, 25);">* June 2019:</b> Our paper <b>Look Further to Recognize Better: Learning Shared Topics and Category-Specific
				 Dictionaries for Open-Ended 3D Object Recognition</b> got accepted at <b><a href="https://www.iros2019.org/" target="_blank"  class="external text" title="https://www.iros2019.org/" rel="nofollow" style="color: rgb(0,0, 255);">IROS2019</a></b>. 

			<br /><b style="color: rgb(0, 150, 25);">* May 2019:</b> We will organize a full-day workshop on <b><a href="https://lcas.lincoln.ac.uk/wp/tig-ii/" target="_blank" class="external text" rel="nofollow" style="color: rgb(0,0, 255);"> Task-Informed Grasping (TIG-II): From Perception to Physical Interaction</a> </b> at <b><a href="http://www.roboticsconference.org/" class="external text" title="http://www.roboticsconference.org/" target="_blank"  rel="nofollow" style="color: rgb(0,0, 255);">RSS2019</a></b>. 

			<br /><b style="color: rgb(0, 150, 25);">* April 2019:</b> We will organize a full-day workshop on <b><a href="http://www.ai.rug.nl/oel/" target="_blank" class="external text" rel="nofollow" style="color: rgb(0,0, 255);"> Open-Ended Learning for Object Perception and Grasping: 
				Current Successes and Future Challenges</a></b> at <b><a href="https://www.iros2019.org/" target="_blank" class="external text" title="https://www.iros2019.org/" target="_blank"  rel="nofollow" style="color: rgb(0,0, 255);">IROS2019</a></b>. 

			<br /><b style="color: rgb(0, 150, 25);">* April 2019:</b> I am honored to be a member of the 2019 cohort of the <b><a href="http://www.roboticsconference.org/" class="external text" rel="nofollow" style="color: rgb(0,0, 255);"> RSS Pioneers</a></b>.

			<br /><b style="color: rgb(0, 150, 25);">* January 2019:</b> Our paper <b>Interactive Open-Ended Object, Affordance and Grasp Learning for Robotic Manipulation</b> got accepted at <b><a href="https://www.icra2019.org/" class="external text" title="https://www.icra2019.org/" rel="nofollow" style="color: rgb(0,0, 255);">ICRA2019</a></b>. 

			<br /><b style="color: rgb(0, 150, 25);">* November 2018:</b> I got the qualification of <b style="color: rgb(0, 0, 255);">University Teaching Skills </b> from the <b> <a href="http://rug.nl/" target="_blank" style="color: rgb(0,0, 255);"> University of Groningen</a></b>, the Netherlands.

			<br /><b style="color: rgb(0, 150, 25);">* October 2018:</b> My <b style="color: rgb(0, 0, 255);">NVIDIA GPU Grant</b> has been approved. Thank NVIDIA Corporation for supporting our works.

			<br /><b style="color: rgb(0, 150, 25);">* July 2018:</b> Our paper <b>Coping with Context Change in Open-Ended Object Recognition without Explicit Context Information</b> got accepted at <b><a href="https://www.iros2018.org/" class="external text" title="https://www.iros2018.org/" rel="nofollow" style="color: rgb(0,0, 255);">IROS2018</a></b>.  -->

			<!-- <br /><b style="color: rgb(0, 150, 25);">* July 2018:</b> I will be a Faculty of Science and Engineering (FSE Fellow) at Artificial Intelligence &amp; Cognitive Engineering, Artificial Intelligence department, <b><a href="https://www.rug.nl/" class="external text" title="https://www.rug.nl/" rel="nofollow" style="color: rgb(0, 0, 255);">University of Groningen</a></b>, the Netherlands. 

			<br /><b style="color: rgb(0, 150, 25);">* December 2017:</b> <b style="color: rgb(0, 0, 255);">Bin-Picking Synthetic Dataset</b> is now available <b><a href="https://goo.gl/BSr2mU" class="external text" title="https://goo.gl/BSr2mU" rel="nofollow" style="color: rgb(0, 0, 255);"> here!</a></b> This dataset contains RGB and depth images captured from multiple views in five different physically feasible bin picking scenarios.

			<br /><b style="color: rgb(0, 150, 25);">* November 2017:</b> Our paper <b>Perceiving, Learning, and Recognizing 3D Objects: An Approach to Cognitive Service Robots</b> got accepted at <b><a href="https://aaai.org/Conferences/AAAI-18/" class="external text" title="https://aaai.org/Conferences/AAAI-18/" rel="nofollow" style="color: rgb(0, 0, 255);" style="color: rgb(0,0, 255);">AAAI2018</a></b>.

			<br /><b style="color: rgb(0, 150, 25);">* June 2017:</b> An open-source implementation of the <b style="color: rgb(0, 0, 255);" >GOOD descriptor</b> is now available <b><a href="https://github.com/SeyedHamidreza/GOOD_descriptor" class="external text" title="https://github.com/SeyedHamidreza/GOOD_descriptor" rel="nofollow" style="color: rgb(0, 0, 255);"> here!</a></b>

			<br /><b style="color: rgb(0, 150, 25);">* May 2017:</b> <b style="color: rgb(0, 0, 255);">Restaurant Object Dataset v.1.0</b> (RGB-D) is now available <b><a href="https://goo.gl/64IXx9" class="external text" title="https://goo.gl/64IXx9" rel="nofollow" style="color: rgb(0, 0, 255);"> here!</a></b> It contains 306 views of one instance of each category (Bottle, Bowl, Flask, Fork, Knife, Mug, Plate, Spoon, Teapot, and Vase), and 31 views of Unknown objects views (e.g. views that belong to the furniture). 

			<br /><b style="color: rgb(0, 150, 25);">* April 2017:</b> New journal paper accepted at <b> <a href="https://www.journals.elsevier.com/neurocomputing" class="external text" title="https://www.journals.elsevier.com/neurocomputing" rel="nofollow" style="color: rgb(0, 0, 255);" >Neurocomputing journal</a></b>: <b>Towards Lifelong Assistive Robotics: A Tight Coupling between Object Perception and Manipulation</b>. 

			<br /><b style="color: rgb(0, 150, 25);">* Jan 2017:</b> I will be a research intern at <b><a href="https://labicvl.github.io" class="external text" title="https://labicvl.github.io" rel="nofollow" style="color: rgb(0, 0, 255);" >ICVL Lab, Imperial Colledge London, UK</a></b>. -->
		</p>
									
</section>


<!-- Three -->
<section id="research">
	<div class="container">
		<h3 style="color: rgb(0, 0, 0);">Research & Publication</h3>
		<p style="text-align: justify; color: rgb(0, 0, 0);">
			My research interests focus on the intersection of <b style="color: rgb(0, 0, 0);"> robotics</b>, <b style="color: rgb(0, 0, 0);">machine learning</b> 
			and <b style="color: rgb(0, 0, 0);">machine vision</b>. I am interested in developing algorithms for an adaptive perception system based on interactive 
			environment exploration and open-ended learning, which enables robots to learn from past experiences and interact with human users. I have been investigating 
			on <b style="color: rgb(0, 0, 0);">  active perception</b>, where robots use their mobility and manipulation capabilities not only to gain the most useful perceptual 
			information to model the world, but also to predict the next best view for improving detection and manipulation performances. I have evaluated my works on
			 different platforms including PR2, robotic arms, and humanoid robots. Details of my publications, including paper, demonstration, and code, can be found 
			 <b style="color: rgb(0, 0, 255);"><i><a href="https://www.ai.rug.nl/irl-lab/publications.html">here!</a></i></b> My up-to-date list of publications can be found on my <b> <a  href="https://scholar.google.com/citations?user=VFr_XuYAAAAJ&hl=en" target="_blank" style="color: rgb(0, 0, 255);">  Google scholar account </a></b>.
			</p>

					
			<p style="text-align: justify; color: rgb(0, 0, 0);">
				In our research group (IRL-Lab), we focus on <b style="color: rgb(0, 0, 255);"><a href="https://www.ai.rug.nl/irl-lab/"> Lifelong Interactive Robot Learning</a></b> to make robots capable of
				learning in an open-ended fashion by interacting with non-expert human users. In particular, we have been developing this goal 
				over the following six specific research directions.  
			</p>

			
	
			<ul class="actions">
				<li><a href="https://www.ai.rug.nl/irl-lab/publications.html" class="button">Publications</a></li>
				<li><a href="https://www.ai.rug.nl/irl-lab/" class="button">IRL-Lab webpage</a></li>
			</ul>
   

<!-- ***************************************************** -->
<!-- ***************************************************** -->
<!-- ***************************************************** -->
<div class="features">
	 
	<article>
		
		<a href="https://www.ai.rug.nl/irl-lab/publications.html" target="_blank" class="image" ><img src="videos/good.gif" alt="" width="100%"/></a>
		<div class="inner">
			<h4 style="text-align: justify; color: rgb(0, 0, 250);" >Perception and Perceptual Learning</h4>
			<p style="text-align: justify; color: rgb(0, 0, 0);">
				We are interested in attaining a 3D understanding of the world around us. In particular, 
				the perception system provides important information that the robot has to use for interacting with users and
				environments. 
			</p>
		</div>
	</article>
	<!-- ***************************************************** -->

	<article>
		<a href="https://www.ai.rug.nl/irl-lab/publications.html" target="_blank" class="image"><img src="videos/serve_a_coke.gif" alt="" /></a>
		<div class="inner">
			<h4 style="text-align: justify; color: rgb(0, 0, 250);" >Object Grasping and Object Manipulation</h4>

			<p style="text-align: justify; color: rgb(0, 0, 0);">
				A service robot must be able to interact with the environment as well as human users. 
				We are interested in fundamental research in object-agnostic grasping, affordance detection, task-informed grasping, and object manipulation.
			</p>
		</div>
	</article>
	<!-- ***************************************************** -->	
	
	<article>
			<a href="https://www.ai.rug.nl/irl-lab/publications.html" target="_blank" class="image"><img src="images/lifelong.png" alt="" /></a>
			<div class="inner">
				<h4 style="text-align: justify; color: rgb(0, 0, 250);" >Lifelong Interactive Robot Learning</h4>
			
				<p style="text-align: justify; color: rgb(0, 0, 0);">
					A service robot must be able to interact with the environment as well as human users. 
					We are interested in fundamental research in object-agnostic grasping, affordance detection, task-informed grasping, and object manipulation.
				</p>
			</div>
	</article>
	<!-- ***************************************************** -->	
	<article>
			<a href="https://www.ai.rug.nl/irl-lab/publications.html" target="_blank" class="image"><img src="videos/dual_arm_video.gif" alt="" /></a>
			<div class="inner">

				<h4 style="text-align: justify; color: rgb(0, 0, 250);" >Dual-Arm Manipulation</h4>
			
				<p style="text-align: justify; color: rgb(0, 0, 0);">
					A dual-arm robot has very good manipulability and maneuverability which is necessary
					to accomplish a set of everyday tasks (dishwashing, hammering).
					We are interested in efficient imitation learning, collabrative manipulation, and large object manipulation. 
				</p>
			</div>
	</article>
	<!-- ***************************************************** -->
	<article>
			<a href="https://www.ai.rug.nl/irl-lab/publications.html" target="_blank" class="image"><img src="videos/neural_motion_planning.gif" alt="" /></a>
			<div class="inner">
									
				<h4 style="text-align: justify; color: rgb(0, 0, 250);" >Dynamic Robot Motion Planning</h4>

				<p style="text-align: justify; color: rgb(0, 0, 0);">
					We are interested in attaining fully reactive manipulation functionalities in a closed-loop manner. 
					Reactive systems have to continuously check if they are at risk of colliding while planners should check every configuration that the robot may attempt to use.
				</p>
			</div>
	</article>
	<!-- ***************************************************** -->
	<article>
			<a href="https://www.ai.rug.nl/irl-lab/publications.html" target="_blank" class="image"><img src="videos/pr2.gif" alt="" /></a>
			<div class="inner">

				<h4 style="text-align: justify; color: rgb(0, 0, 250);" >Exploiting Multimodality</h4>

				<p style="text-align: justify; color: rgb(0, 0, 0);">
					A service robot may sense the world through different modalities that may provide visual, haptic or auditory cues about the environment.  
					In this vein, we are interested in exploiting multimodality for learning better representations to improve robot's performance.
				</p>
			</div>
	</article>
	<!-- ***************************************************** -->
	

</div>

<!-- ***************************************************** -->
<!-- ***************************************************** -->
<!-- ***************************************************** -->
<div class="features">
	 
<article>
	<h4 style="text-align: justify; color: rgb(0, 0, 250);" >OrthographicNet: A Deep Transfer Learning Approach for 3D Object Recognition in Open-Ended Domains</h4>
		<a href="https://youtu.be/DoqjU6GaqAQ" target="_blank" class="image"><img src="images/orthographicNet.jpg" alt="" /></a>
		<div class="inner">
									
		<p style="text-align: justify; color: rgb(0, 0, 0);">
			We present OrthographicNet, a deep 
			transfer learning based approach, for 3D object recognition in open-ended domains. In particular, OrthographicNet generates a
			rotation and scale invariant global feature for a given object, enabling to recognize the same or similar objects seen from 
			different perspectives. Experimental results show that our approach yields significant improvements over the state-of-the-art 
			approaches concerning scalability, memory usage and object recognition performance.
			<li><a class="icon fa-file-pdf-o" href="https://ieeexplore.ieee.org/document/9311749" target="_blank"> IEEE TMECH (2020) </a></li>
			<li><a class="icon fa-file-pdf-o" href="https://arxiv.org/abs/1902.03057" target="_blank"> Preprint on arXiv (2019)</a></li>			
			<li><a class="icon style3 major fa-camera-retro" href="https://youtu.be/DoqjU6GaqAQ" target="_blank"> Demo1</a> --- <a class="icon style3 major fa-camera-retro" href="https://youtu.be/5hcNGQNi1L8" target="_blank"> Demo2</a> 
			</li>
			<!-- <p><iframe width="640" height="360" src="https://www.youtube.com/embed/Hli1ky0bgT4?rel=0" frameborder="0" allowfullscreen=""></iframe> </p>-->
		</p>
		</div>
	</article>

	<!-- ***************************************************** -->
<!-- ***************************************************** -->
<!-- ***************************************************** -->
	<article>
		<h4 style="text-align: justify; color: rgb(0, 0, 250);" >Simultaneous Multi-View Object Grasping and Recognition in Open-Ended Domains</h4>
		<a class="image"><img src="images/mvgrasp.jpg" alt="" /></a>
		<div class="inner">
									
		<p style="text-align: justify; color: rgb(0, 0, 0);">
		 Most state-of-the-art approaches tackle object recognition and grasping as two separate problems while both use visual input.
		 Such approaches are not suitable for task-informed grasping, where the robot should recognize a specific object first and then
		 grasp and manipulate it to accomplish a task. In this work, we propose a multi-view deep learning approach to handle simultaneous 
		 object grasping and recognition in open-ended domains. 
		 In particular, our approach takes multi-view of the object as input and jointly estimates pixel-wise grasp configuration and 
		 a deep scale- and rotation-invariant representation. The obtained representation is then used for open-ended object category learning and recognition. Experimental
		 results on benchmark datasets have shown that our approach outperforms state-of-the-art methods by a large margin in terms
		 of grasping and recognition.
 
			<!-- <li><a class="icon fa-file-pdf-o" href="https://ieeexplore.ieee.org/document/9311749" target="_blank"> IEEE TMECH (2020) </a></li> -->
			<li><a class="icon fa-file-pdf-o" href="" target="_blank"> Preprint on arXiv (2021)</a></li>			
			<!-- <li><a class="icon style3 major fa-camera-retro" href="https://youtu.be/DoqjU6GaqAQ" target="_blank"> Demo1</a> --- <a class="icon style3 major fa-camera-retro" href="https://youtu.be/5hcNGQNi1L8" target="_blank"> Demo2</a>  -->
			</li>
			<!-- <p><iframe width="640" height="360" src="https://www.youtube.com/embed/Hli1ky0bgT4?rel=0" frameborder="0" allowfullscreen=""></iframe> </p>-->
		</p>
		</div>
	</article>
 
<!-- ***************************************************** -->
<!-- ***************************************************** -->
<!-- ***************************************************** -->
	<article>
	<h4 style="text-align: justify; color: rgb(0, 0, 250);" >Investigating the Importance of Shape Features, Color Constancy, Color Spaces and Similarity Measures in Open-Ended 3D Object Recognition</h4>
		<a href="https://youtu.be/eNdIMWj9ido" target="_blank" class="image"><img src="images/color_constancy.png" alt="" /></a>
		<div class="inner">
									
		<p style="text-align: justify; color: rgb(0, 0, 0);">
				Despite the recent success of state-of-the-art 3D object recognition approaches, service robots are frequently failed to recognize many objects in real human-centric environments. 
				<!-- For these robots, object recognition is a challenging task due to the high demand for accurate and real-time response under changing and unpredictable environmental conditions. -->
				Most of the recent approaches use either the shape information only and ignore the role of color information or vice versa. 
				Furthermore, they mainly utilize the Ln Minkowski family functions to measure the similarity of two object views, while there are various distance measures 
				that are applicable to compare two object views. In this paper, we explore the importance of shape information, color constancy, color spaces, and various similarity measures 
				in open-ended 3D object recognition. 
			<li><a class="icon fa-file-pdf-o" href="https://link.springer.com/article/10.1007/s11370-021-00349-8" target="_blank"> Intelligent Service Robotics (open access)</a></li>
			<li><a class="icon fa-file-pdf-o" href="https://arxiv.org/abs/2002.03779" target="_blank"> Preprint on arXiv (2020)</a></li>
			<li><a class="icon style3 major fa-camera-retro" href="https://youtu.be/eNdIMWj9ido" target="_blank"> Demo</a> 
			</li>
			<!-- <p><iframe width="640" height="360" src="https://www.youtube.com/embed/Hli1ky0bgT4?rel=0" frameborder="0" allowfullscreen=""></iframe> </p>-->
		</p>
	</div>
	</article>

<!-- ***************************************************** -->
<!-- ***************************************************** -->
<!-- ***************************************************** -->
<article>
	<h4 style="text-align: justify; color: rgb(0, 0, 250);" >Local-HDP: Interactive Open-Ended 3D Object Categorization</h4>
		<a href="https://youtu.be/otxd8D8yYLc" target="_blank" class="image"><img src="images/localHDP2.jpg" alt="" /></a>
		<div class="inner">
									
		<p style="text-align: justify; color: rgb(0, 0, 0);">
			We introduce a non-parametric hierarchical Bayesian approach for open-ended 3D object categorization, named the Local Hierarchical Dirichlet Process (Local-HDP). This method allows an agent to
			learn independent topics for each category incrementally and to adapt to the environment in time. Hierarchical Bayesian approaches like Latent Dirichlet Allocation (LDA) can transform low-level
			features to high-level conceptual topics for 3D object categorization. However, the efficiency and accuracy of LDA-based approaches depend on the number of topics that is chosen manually. Moreover,
			fixing the number of topics for all categories can lead to overfitting or underfitting of the model. In contrast, the proposed Local-HDP can autonomously determine the number of topics for each
			category. Furthermore, an inference method is proposed that results in a fast posterior approximation. 
			<!-- Experiments show that Local-HDP outperforms other state-of-the-art approaches in terms of accuracy, scalability, and memory efficiency with a large margin. -->
			<li><a class="icon fa-file-pdf-o" href="https://arxiv.org/abs/2009.01152" target="_blank"> Preprint on arXiv (2020)</a></li>
			<li><a class="icon style3 major fa-camera-retro" href="https://youtu.be/otxd8D8yYLc" target="_blank"> Demo</a> 
			</li>
			<!-- <p><iframe width="640" height="360" src="https://www.youtube.com/embed/Hli1ky0bgT4?rel=0" frameborder="0" allowfullscreen=""></iframe> </p>-->
		</p>
	</div>
	</article>

<!-- ***************************************************** -->
<!-- ***************************************************** -->
<!-- ***************************************************** -->
	<article>
	<h4 style="text-align: justify; color: rgb(0, 0, 250);" >The State of Service Robots: Current Bottlenecks in Object Perception and Manipulation</h4>
		<a class="image"><img src="images/modules.jpg" alt="" /></a>
		<div class="inner">
									
		<p style="text-align: justify; color: rgb(0, 0, 0);">
			Nowadays, robots are able to recognize various objects, and quickly plan a collision-free trajectory to grasp a target object. 
			While there are many successes, the robot should be painstakingly coded in advance to perform a set of predefined tasks. Besides, 
			in most of the cases, there is a reliance on large amounts of training data. 
			<!-- Therefore, the knowledge of such robots is fixed after 
			the training phase, and any changes in the environment require complicated, time-consuming, and expensive robot re-programming by human experts.  -->
			Therefore, 
			these approaches are still too rigid for real-life applications in unstructured environments, where a significant portion of the environment 
			is unknown and cannot be directly sensed or controlled. In this paper, we review advances in service robots from object perception to complex object manipulation
			 and shed a light on the current challenges and bottlenecks.
			<li><a class="icon fa-file-pdf-o" href="https://arxiv.org/abs/2003.08151" target="_blank"> Preprint on arXiv (2020)</a></li>
			<!-- <p><iframe width="640" height="360" src="https://www.youtube.com/embed/Hli1ky0bgT4?rel=0" frameborder="0" allowfullscreen=""></iframe> </p>-->
		</p>
	</div>
	</article>
<!-- ***************************************************** -->
<!-- ***************************************************** -->
<!-- ***************************************************** -->

	<article>
		<h4 style="text-align: justify; color: rgb(0, 0, 250);" >Accelerating Reinforcement Learning for Reaching using Continuous Curriculum Learning</h4>
			<a href="https://youtu.be/WY-1EbYBSGo" target="_blank" class="image"><img src="images/IJCNN.png" alt="" /></a>
			<div class="inner">
										
			<p style="text-align: justify; color: rgb(0, 0, 0);">
					<!-- Reinforcement learning has shown great promise in the training of robot behavior due to the sequential decision making characteristics.  -->
					<!-- However, the required enormous amount of interactive and informative training data provides the major stumbling block for progress.  -->
					In this study, we focus on accelerating reinforcement learning (RL) training and improving the performance of multi-goal reaching tasks. 
					Specifically, we propose a precision-based continuous curriculum learning (PCCL) method in which the requirements are gradually adjusted 
					during the training process, instead of fixing the parameter in a static schedule. To this end, we explore various continuous curriculum strategies 
					for controlling a training process. This approach is tested using a Universal Robot 5e in both simulation and real-world scenarios. 					
		<li><a class="icon fa-file-pdf-o" href="https://arxiv.org/abs/2002.02697" target="_blank"> Preprint on arXiv (2020)</a> </li>
		<li><a class="icon fa-file-pdf-o" href="https://ieeexplore.ieee.org/document/9207427" target="_blank"> IJCNN conference </a></li>
		<li><a class="icon style3 major fa-camera-retro" href="https://youtu.be/WY-1EbYBSGo" target="_blank"> Demo1</a> 
		</li>
		<!-- <p><iframe width="640" height="360" src="https://www.youtube.com/embed/Hli1ky0bgT4?rel=0" frameborder="0" allowfullscreen=""></iframe> </p>-->
		</p>
		</div>
	</article>

<!-- ***************************************************** -->
<!-- ***************************************************** -->
<!-- ***************************************************** -->

<article>
<h4 style="text-align: justify; color: rgb(0, 0, 250);" >Look Further to Recognize Better: Learning Shared Topics and Category-Specific Dictionaries for Open-Ended 3D Object Recognition</h4>
	<a href="https://youtu.be/icsC_2rQBNo" target="_blank" class="image"><img src="images/fine_grained.jpg" alt="" /></a>
	<div class="inner">
								
	<p style="text-align: justify; color: rgb(0, 0, 0);">
		In human-centric environments, fine-grained object categorization is as essential as basic-level object categorization. In this work, each object is 
		represented using a set of general latent topics and category-specific dictionaries. The general topics encode the common patterns of all categories, 
		while the category-specific dictionary describes the content of each category in details. We discovered both sets of general and specific representations
		in an unsupervised fashion and updated them incrementally using new object views. 
		<li><a class="icon fa-file-pdf-o" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8967823" target="_blank"> IROS2019</a> </li> 
		<li><a class="icon fa-file-pdf-o" href="https://arxiv.org/abs/1907.12924" target="_blank"> Preprint on arXiv (2019)</a>  </li> 

		<li><a class="icon style3 major fa-camera-retro" href="https://youtu.be/zjucGaAwnTE" target="_blank"> Demo1</a> --- <a class="icon style3 major fa-camera-retro" href="https://youtu.be/icsC_2rQBNo" target="_blank"> Demo2</a>
		</li>
		<!-- <p><iframe width="640" height="360" src="https://www.youtube.com/embed/Hli1ky0bgT4?rel=0" frameborder="0" allowfullscreen=""></iframe> </p>-->
	</p>
	</div>
</article>


<!-- ***************************************************** -->
<!-- ***************************************************** -->
<!-- ***************************************************** -->

<article>
<h4 style="text-align: justify; color: rgb(0, 0, 250);" >Interactive Open-Ended Learning Approach for Recognizing 3D Object Category and Grasp Affordance Concurrently</h4>
	<a href="https://youtu.be/MrqmnBbXc70" target="_blank"  class="image"><img src="images/grasping.jpg" alt="" /></a>
	<div class="inner">
								
	<p style="text-align: justify; color: rgb(0, 0, 0);">
This paper presents an interactive open-ended learning approach to recognize multiple objects and their grasp affordances concurrently. This is an important
contribution in the field of service robots since no matter how extensive the training data used for batch learning, a robot might always be confronted with an unknown object when operating in human-centric environments. Our approach has two main branches. The first branch is related to open-ended 3D object category learning and recognition. The second branch is associated with learning and recognizing the configuration of grasps in a reasonable amount of time.  
<li><a class="icon fa-file-pdf-o" href="https://ieeexplore.ieee.org/document/8794184" target="_blank"> ICRA2019</a> </li>
<li><a class="icon fa-file-pdf-o" href="https://arxiv.org/abs/1904.02530" target="_blank"> Preprint on arXiv (2019)</a></li>
<li><a class="icon style3 major fa-camera-retro" href="https://youtu.be/yM6nGk_qGHk" target="_blank"> Demo1</a> - <a class="icon style3 major fa-camera-retro" href="https://youtu.be/jYbjGKG4c-U" target="_blank"> Demo2</a> 
- <a class="icon style3 major fa-camera-retro" href="https://youtu.be/MrqmnBbXc70" target="_blank"> Demo3</a> - <a class="icon style3 major fa-camera-retro" href="https://youtu.be/HoEjJJOynmY" target="_blank"> Demo4</a> 

</li>
</p>
</div>
</article>
<!-- ***************************************************** -->
<!-- ***************************************************** -->
<!-- ***************************************************** -->
<article>
	<h4 style="text-align: justify; color: rgb(0, 0, 250);" >Learning to Grasp 3D Objects using Deep Residual U-Nets</h4>
		<a href="https://youtu.be/5_yAJCc8owo" target="_blank" class="image"><img src="images/yikun_paper.png" alt="" /></a>
		<div class="inner">
									
		<p style="text-align: justify; color: rgb(0, 0, 0);">
				In this study, we present a new deep learning approach to detect object affordances for a given 3D object. The method trains a Convolutional
				Neural Network (CNN) to learn a set of grasping features from RGB-D images. We named our approach Res-U-Net since the architecture of the network 
				is designed based on U-Net structure and residual network-styled blocks. It devised to be robust and efficient to compute and use. A set of experiments
				has been performed to assess the performance of the proposed approach regarding grasp success rate on simulated robotic scenarios. 
				<!-- Experiments validate the promising performance of the proposed architecture on ShapeNetCore dataset and simulated robot scenarios.  -->
				<li><a class="icon fa-file-pdf-o" href="https://ieeexplore.ieee.org/abstract/document/9223541" target="_blank"> RO-MAN2020 </a></li>
				<li><a class="icon fa-file-pdf-o" href="https://arxiv.org/abs/2002.03892" target="_blank"> Preprint on arXiv (2020)</a> </li>
			

			<li><a class="icon style3 major fa-camera-retro" href="https://youtu.be/5_yAJCc8owo" target="_blank"> Demo1</a> 
			</li>
		</p>
		</div>
	</article>
<!-- ***************************************************** -->
<!-- ***************************************************** -->
<!-- ***************************************************** -->
<article>
<h4 style="text-align: justify; color: rgb(0, 0, 250);" >Coping with Context Change in Open-Ended Object Recognition without Explicit Context Information</h4>
	<a href="#" class="image"><img src="images/pic01.jpg" alt="" /></a>
	<div class="inner">
								
	<p style="text-align: justify; color: rgb(0, 0, 0);">
One of the main challenges in unconstrained human environments is to cope with the effects of context change. This paper presents two main contributions: (<b>1</b>) an approach for evaluating open-ended object category learning and recognition methods in multi-context scenarios; (<b>2</b>) evaluation of different object category learning and recognition approaches regarding their ability to cope with the effects of context change. 
<li><a class="icon fa-file-pdf-o" href="https://ieeexplore.ieee.org/document/8593922" target="_blank"> IROS2018 </a></li>
<li><a class="icon style3 major fa-camera-retro" href="https://youtu.be/l6q6fI5H6zY" target="_blank"> Demo: multi contexts open-ended scenario</a></li>
</p>
</div>
</article>

<!-- ***************************************************** -->
<!-- ***************************************************** -->
<!-- ***************************************************** -->

<article>
<h4 style="text-align: justify; color: rgb(0, 0, 250);" >Perceiving, Learning, and Recognizing 3D Objects: An Approach to Cognitive Service Robots</h4>
<a href="#" class="image"><img src="images/AAAI-ICCV.jpg" alt="" /></a>
<div class="inner">

<p style="text-align: justify; color: rgb(0, 0, 0);">
This paper proposes a cognitive architecture designed to create a concurrent 3D object category learning and recognition in an interactive and open-ended manner. In particular, this cognitive architecture provides automatic perception capabilities that will allow robots to detect objects in highly crowded scenes and learn new object categories from the set of accumulated experiences in an incremental and open-ended way. Moreover, it supports constructing the full model of an unknown object in an on-line manner and predicting next best view for improving object detection and manipulation performance.
<li><a class="icon fa-file-pdf-o" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17389" target="_blank"> AAAI2018</a></li>
<li><a class="icon style3 major fa-camera-retro" href="https://youtu.be/CuBS2L2q5NU" target="_blank"> Demo1</a> ---
<a class="icon style3 major fa-camera-retro" href="https://youtu.be/LZtI-s95uTk" target="_blank"> Demo2</a> ---
<a class="icon style3 major fa-camera-retro" href="https://youtu.be/eP0lwqW55Iw" target="_blank"> Demo3</a></li>
</p>
</div>
</article>
<!-- ***************************************************** -->
<!-- ***************************************************** -->
<!-- ***************************************************** -->

<article>
<h4 style="text-align: justify; color: rgb(0, 0, 250);" >Active Multi-View 6D Object Pose Estimation and Camera Motion Planning in the Crowd</h4>
<a href="#" class="image"><img src="images/Bin-picking-dataset.jpg" alt="" /></a>
<div class="inner">

<p style="text-align: justify; color: rgb(0, 0, 0);">
In this project, we developed a novel unsupervised Next-Best-View (NBV) prediction algorithm to improve object detection and manipulation performance. Particularly, the ability to predict the NBV point is important for mobile robots performing tasks in everyday environments. In active 
scenarios, whenever the robot fails to detect or manipulate objects from the current view point, it is able to predict the next best view position, goes there and captures a new scene to improve the knowledge of the environment. This may increase the object detection and manipulation performance.
<li><a class="icon fa-file-pdf-o" href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/w31/html/Sock_Multi-View_6D_Object_ICCV_2017_paper.html" target="_blank"> ICCV2017-WS</a></li>
<li><a class="icon fa-file-code-o" href="https://goo.gl/BSr2mU" target="_blank"> Bin-Picking Synthetic Dataset (RGB-D)</a>
</p>
</div>
</article>

<!-- ***************************************************** -->
<!-- ***************************************************** -->
<!-- ***************************************************** -->

<article>
<h4 style="text-align: justify; color: rgb(0, 0, 250);" >Hierarchical Object Representation for OpenEnded Object Category Learning and Recognition (Local LDA)</h4>
<a href="#" class="image"><img src="images/PR2_LDA.jpg" alt="" /></a>
<div class="inner">

<p style="text-align: justify; color: rgb(0, 0, 0);">
This paper proposes an open-ended 3D object recognition system which concurrently learns both the object categories and the statistical features for encoding objects. In particular, we propose an extension of Latent Dirichlet Allocation to learn structural semantic features (i.e. topics), from low-level feature co-occurrences, for each category independently. Moreover, topics in each category are discovered in an unsupervised fashion and are updated incrementally using new object views. In this way, the advantage of both the local hand-crafted and the structural semantic features have been considered in an efficient way. 

<li><a class="icon fa-file-pdf-o" href="https://papers.nips.cc/paper/6539-hierarchical-object-representation-for-open-ended-object-category-learning-and-recognition" target="_blank"> NIPS2016</a> --- 
<a class="icon fa-file-pdf-o" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" target="_blank"> TPAMI (To appear)</a>
<li><a class="icon style3 major fa-camera-retro" href="https://youtu.be/J0QOc_Ifde4" target="_blank"> Demo1</a> ---
<a class="icon style3 major fa-camera-retro" href="https://youtu.be/pe29DYNolBE" target="_blank"> Demo2</a>
</p>
</div>
</article>

<!-- ***************************************************** -->
<!-- ***************************************************** -->
<!-- ***************************************************** -->


<article>
<h4 style="text-align: justify; color: rgb(0, 0, 250);" >GOOD: A Global Orthographic Object Descriptor for 3D Object Recognition and Manipulation</h4>
<a href="#" class="image"><img src="images/GOOD.jpg" alt="" /></a>
<div class="inner">

<p style="text-align: justify; color: rgb(0, 0, 0);">
The Global Orthographic Object Descriptor (<b>GOOD</b>) has been designed to be robust, descriptive and efficient to compute and use. GOOD descriptor has two outstanding characteristics: (<b>1</b>) Providing a good trade-off among: <b>descriptiveness</b>, <b>robustness</b>, <b>computation time</b>, <b>memory usage</b>; (<b>2</b>) Allowing <b>concurrent object recognition and pose estimation for manipulation</b>.  The performance of the proposed object descriptor is compared with the main state-of-the-art descriptors. Experimental results show that the overall classification performance obtained with GOOD is comparable to the best performances obtained with the state-of-the-art descriptors. Concerning memory and computation time, GOOD clearly outperforms the other descriptors.  The current implementation of GOOD descriptor supports several functionalities for 3D object recognition and object manipulation.

<li><a class="icon fa-file-pdf-o" href="http://www.sciencedirect.com/science/article/pii/S0167865516301684" target="_blank"> Pattern Recognition Letters</a> --- 
<a class="icon fa-file-pdf-o" href="http://ieeexplore.ieee.org/abstract/document/7759612/" target="_blank"> IROS2016</a>
<li><a class="icon style3 major fa-camera-retro" href="https://youtu.be/iEq9TAaY9u8" target="_blank"> Demo1</a> ---
<a class="icon style3 major fa-camera-retro" href="https://youtu.be/eP0lwqW55Iw" target="_blank"> Demo2</a>
<li><a class="icon fa-file-code-o" href="https://github.com/SeyedHamidreza/GOOD_descriptor" target="_blank"> Source Code (GitHub)</a> ---
<a class="icon fa-file-code-o" href="http://pointclouds.org/" target="_blank"> Part of PCL 1.9</a> 
</li>
</p>
</div>
</article>

<!-- ***************************************************** -->
<!-- ***************************************************** -->
<!-- ***************************************************** -->


<article>
<h4 style="text-align: justify; color: rgb(0, 0, 250);" >Towards Lifelong Assistive Robotics: A Tight Coupling between Object Perception and Manipulation</h4>
<a href="#" class="image"><img src="images/grasp.jpg" alt="" /></a>
<div class="inner">

<p style="text-align: justify; color: rgb(0, 0, 0);">
In this work, we propose a cognitive architecture designed to create a tight coupling between perception and manipulation for assistive robots. This is necessary for assistive robots, not only to perform manipulation tasks in a reasonable amount of time and in an appropriate manner, but also to robustly adapt to new environments by handling new objects. In particular, this cognitive architecture provides perception capabilities that will allow robots to, incrementally learn object categories from the set of accumulated experiences and reason about how to perform complex tasks. 

<li><a class="icon fa-file-pdf-o" href="https://www.sciencedirect.com/science/article/pii/S0925231218302327" target="_blank"> Neurocomputing Journal</a> --- 
<a class="icon fa-file-pdf-o" href="http://www.ais.uni-bonn.de/robocup.de/2016/papers/RoboCup_Symposium_2016_Kasaei.pdf" target="_blank"> RoboCup2016</a> --- 
<a class="icon fa-file-pdf-o" href="http://ieeexplore.ieee.org/document/7353715/" target="_blank"> IROS2015</a>
<li><a class="icon style3 major fa-camera-retro" href="https://www.youtube.com/watch?v=cTK10iNyYXg" target="_blank"> Demo1</a> ---
<a class="icon style3 major fa-camera-retro" href="https://youtu.be/GtXBiejdccw" target="_blank"> Demo2</a> ---
<a class="icon style3 major fa-camera-retro" href="https://youtu.be/MwX3J6aoAX0" target="_blank"> Demo3</a>
</p>
</div>
</article>

<!-- ***************************************************** -->
<!-- ***************************************************** -->
<!-- ***************************************************** -->


<article>
<h4 style="text-align: justify; color: rgb(0, 0, 250);" >Interactive Open-Ended Learning for 3D Object Recognition: An Approach and Experiments</h4>
<a href="#" class="image"><img src="images/HRI.jpg" alt="" /></a>
<div class="inner">

<p style="text-align: justify; color: rgb(0, 0, 0);">
This work presents an efficient approach capable of learning and recognizing object categories in an interactive and open-ended manner. In particular, we mainly focus on two state-of-the-art questions: (1) How to automatically detect, conceptualize and recognize objects in 3D scenes in an open-ended manner? (2) How to acquire and use high-level knowledge obtained from the interaction with human users, namely when they provide category labels, in order to improve the system performance?


<li><a class="icon fa-file-pdf-o" href="https://link.springer.com/article/10.1007/s10846-015-0189-z" target="_blank"> Journal of Intelligent and Robotic Systems</a>-<br/>-- 
<a class="icon fa-file-pdf-o" href="https://www.sciencedirect.com/science/article/pii/S0921889015002146" class="external text" target="_blank"> RAS Journal</a> --- 
<a class="icon fa-file-pdf-o" href="http://ieeexplore.ieee.org/document/6942861/?denied" target="_blank"> IROS2014</a>
<li><a class="icon style3 major fa-camera-retro" href="https://youtu.be/XvnF2JMfhvc" target="_blank"> Demo1</a> ---
<a class="icon style3 major fa-camera-retro" href="https://youtu.be/D00j_rVmtp4" target="_blank"> Demo2</a> 
<li><a class="icon fa-file-code-o" href="https://goo.gl/64IXx9" target="_blank"> Restaurant Object Dataset v.1.0 (RGB-D)</a> 
</p>
</div>
</article>

<!-- ***************************************************** -->
<!-- ***************************************************** -->
<!-- ***************************************************** -->


<article>
<h4 style="text-align: justify; color: rgb(0, 0, 250);" >Learning to Grasp Familiar Objects using Object View Recognition and Template Matching</h4>
<a href="#" class="image"><img src="images/graspSim.jpg" alt="" /></a>
<div class="inner">

<p style="text-align: justify; color: rgb(0, 0, 0); ">
In this work, interactive object view learning and recognition capabilities are integrated in the process of learning and recognizing grasps. The object view recognition module uses an interactive incremental learning approach to recognize object view labels. The grasp pose learning approach uses local and global visual features of a demonstrated grasp to learn a grasp template associated with the recognized object view. A grasp distance measure based on Mahalanobis distance is used in a grasp template matching approach to recognize an appropriate grasp pose. 

<li><a class="icon fa-file-pdf-o" href="http://ieeexplore.ieee.org/document/7759448/" target="_blank"> IROS2016</a></li>
<li><a class="icon style3 major fa-camera-retro" href="https://youtu.be/HoEjJJOynmY" target="_blank"> Demo1</a> ---
<a class="icon style3 major fa-camera-retro" href="https://youtu.be/MrqmnBbXc70" target="_blank"> Demo2</a> </li>
</p>
</div>
</article>


<!-- ***************************************************** -->
<!-- ***************************************************** -->
<!-- ***************************************************** -->


<article>
<h4 style="text-align: justify; color: rgb(0, 0, 250);" >Face Recognition Using Single Normal Reference Image and Statistical Features (Master Thesis)</h4>
<a href="#" class="image"><img src="images/face.jpg" alt="" /></a>
<div class="inner">

<p style="text-align: justify; color: rgb(0, 0, 0); ">
Many types of research have been conducted to improve the accuracy of face recognition techniques. The majority of reported techniques make use of databases where a number of images are available for each person. Since collecting face samples is a challenging task, there are some face recognition methods that work based on a single sample per person (SSPP). I studied face recognition using single normal reference image and statistical features. We encoded the face information by making use of a Modular Principal Component Analysis. The nearest neighbour classifier was finally used to assess the dissimilarity between the target face and trained faces. 
</p>
</div>
</article>

<!-- ***************************************************** -->
<!-- ***************************************************** -->
<!-- ***************************************************** -->


<article>
<h4 style="text-align: justify; color: rgb(0, 0, 250);" > Humanoid Robots (RoboCup-HL)</h4>
<a href="#" class="image"><img src="images/humanoids.jpg" alt="" /></a>
<div class="inner">

<p style="text-align: justify; color: rgb(0, 0, 0); ">
After obtaining extensive knowledge about real-time intelligent robotic systems in Middle-Size League, I tried to make humanoid robots and formed two new robotic teams namely Persia and BehRobot for participating in RoboCup humanoid leagues. We worked on three different types of humanoid robots including kid-size (height = 59cm, weight = 4kg), teen-size (height = 93cm, weight = 7Kg) and adult-size (height = 155cm, weight = 11:5Kg) robots. We were one of the successful teams in the humanoid leagues and achieved several ranks in national and international competitions.
<li><a class="icon fa-file-pdf-o" href="https://ieeexplore.ieee.org/document/7781957/" target="_blank"> ICARSC2016</a></li>
<li><a class="icon style3 major fa-camera-retro" href="https://youtu.be/ko29dpE-A1c?t=66" target="_blank"> Demo1</a> </li>
</p>
</div>
</article>
<!-- ***************************************************** -->
<!-- ***************************************************** -->
<!-- ***************************************************** -->
<article>
<h4 style="text-align: justify; color: rgb(0, 0, 250);" >Middle Size Soccer Robots (RoboCup-MSL)</h4>
<a href="#" class="image"><img src="images/middle.jpg" alt="" /></a>
<div class="inner">

<p style="text-align: justify; color: rgb(0, 0, 0); ">
During the second year of my undergraduate program, I got familiar with RoboCup competitions. I formed a team of Middle Size Soccer Robots (RoboCup-MSL) namely
ADRO in 2006. We provided five player robots and one goalkeeper robot with similar structure but equipped with some additional accessories and sensors. Through this teamwork, I took an active role in the development of the robots’ software. Furthermore, I worked on the mechanical design of the robot via Autodesk Inventor. We achieved several ranks in national and international RoboCup competitions.
<li><a class="icon style3 major fa-camera-retro" href="https://youtu.be/ko29dpE-A1c" target="_blank"> Demo1</a> </li>
</p>
</div>
</article>

<!-- ***************************************************** -->
<!-- ***************************************************** -->
<!-- ***************************************************** -->


<article>
<h4 style="text-align: justify; color: rgb(0, 0, 250);" >A Novel Morphological Method for Detection and Recognition of Vehicle License Plates</h4>
<a href="#" class="image"><img src="images/lpr.jpg" alt="" /></a>
<div class="inner">

	<p style="text-align: justify; color: rgb(0, 0, 0); ">
		License plate detection and recognition is an image-processing technique used to identify a vehicle by its license plate. This notable technology has got multiple
		applications in various traffic and security cases. This study presented a novel method of identifying and recognizing license plates based on the morphology and template matching. The algorithm started with preprocessing and signal conditioning. Next license plate is localized using morphological operators. Then a template matching scheme will be used to recognize the digits and characters within the plate. 
		<li><a class="icon fa-file-pdf-o" href="http://thescipub.com/abstract/10.3844/ajassp.2009.2066.2070" target="_blank"> American Journal of Applied Sciences
		</a> -<br/>-- <a class="icon fa-file-pdf-o" href="https://ieeexplore.ieee.org/document/6061241/" target="_blank"> EISIC2011</a>
		<li><a class="icon fa-file-code-o" href="https://github.com/SeyedHamidreza/car_plate_dataset" target="_blank"> Iranian Car Plate Dataset v.1.0 (RGB)</a>
	</p>
</div>
</article>

</section>
<!-- ***************************************************** -->
<!-- ***************************************************** -->
<!-- ***************************************************** -->

<!-- ^^^^^^^^^^^^^^^^^^^^^^^^^^ Section 4 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ -->
<!-- ^^^^^^^^^^^^^^^^^^^^^^^^^^ Section 4 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ -->
<!-- ^^^^^^^^^^^^^^^^^^^^^^^^^^ Section 4 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ -->
<section id="students">
	<div class="container">	
	<h3 style="color: rgb(0, 0, 0);"> Students</h3>
	<article>
	
		
	<h4>PhD Students</h3>
		<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>

		<div class="box alt">
			<div class="row gtr-50 gtr-uniform">
				<div class="col-5"><span class="image fit"><img src="images/sha.png" alt="" /></span></div>
				<div class="col-7"><span class="image fit"><ul>
						<li style="text-align: justify; color: rgb(0, 0, 0);">Sha Luo (Oct.2018 ~ )</li>
						<li style="text-align: justify;">Deep Reinforcement Learning for Flexible Visually Guided Grasping</li>					
						<li style="text-align: justify;"> Advisors: Lambert Schomaker, Hamidreza Kasaei</li>					
					</ul>
					<!-- <a class="button" >ICRA2021</a> 
					<a class="button" >Demo</a> 
					<a class="button">Thesis</a> -->
					</span>
				</div>
			<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>
		</div>
		<!-- ***************************************************** -->

		<!-- ***************************************************** -->

	</article>

<!-- ^^^^^^^^^^^^^^^^^^^^^^^^^^ MASTER ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ -->
<!-- ^^^^^^^^^^^^^^^^^^^^^^^^^^ MASTER ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ -->
<!-- ^^^^^^^^^^^^^^^^^^^^^^^^^^ mASTER ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ -->

	<h4>Master Students</h3>
	<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>

	<div class="box alt">
		<div class="row gtr-50 gtr-uniform">
			<div class="col-5"><span class="image fit"><img src="images/jos.png" alt="" /></span></div>
			<div class="col-7"><span class="image fit"><ul>
					<li style="text-align: justify; color: rgb(0, 0, 0);">Jos van Goor (Feb.2021 ~ )</li>
					<li style="text-align: justify;">Leveraging Deep Object Recognition Models for Per-Point 6-DOF Grasp Synthesis</li>					
				</ul>
				<!-- <a class="button" >Paper</a> 
				<a class="button" >Demo</a> 
				<a class="button">Thesis</a> -->

				</span>
			</div>
		<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>
	</div>



	<div class="box alt">
		<div class="row gtr-50 gtr-uniform">
			<div class="col-5"><span class="image fit"><img src="images/krishna.png" alt="" /></span></div>
			<div class="col-7"><span class="image fit"><ul>
					<li style="text-align: justify; color: rgb(0, 0, 0);">Krishna Santhakumar (Feb.2021 ~ )</li>
					<li style="text-align: justify;">Lifelong Object Grasp Synthesis using Dual Memory Recurrent Self-Organization Networks</li>					
				</ul>
				<!-- <a class="button" >Paper</a> 
				<a class="button" >Demo</a> 
				<a class="button">Thesis</a> -->

				</span>
			</div>
		<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>
	</div>


	<div class="box alt">
		<div class="row gtr-50 gtr-uniform">
			<div class="col-5"><span class="image fit"><img src="images/arjan.png" alt="" /></span></div>
			<div class="col-7"><span class="image fit"><ul>
					<li style="text-align: justify; color: rgb(0, 0, 0);">Arjan Jawahier (March.2021 ~ )</li>
					<li style="text-align: justify;">Descriptive Viewpoint Prediction: Simultaneous Object Recognition and Grasping in Service Robots</li>					
				</ul>
				<!-- <a class="button" >Paper</a> 
				<a class="button" >Demo</a> 
				<a class="button">Thesis</a> -->

				</span>
			</div>
		<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>
	</div>

	
	<div class="box alt">
		<div class="row gtr-50 gtr-uniform">
			<div class="col-5"><span class="image fit"><img src="images/Hari.png" alt="" /></span></div>
			<div class="col-7"><span class="image fit"><ul>
					<li style="text-align: justify; color: rgb(0, 0, 0);">Hari Vidharth (Feb.2021 ~ )</li>
					<li style="text-align: justify;"> Learning and Generalization of Long-Horizon Sequential Pick and Place Tasks with Deep Reinforcement Learning
					</li>					
				</ul>
				<!-- <a class="button" >Paper</a> 
				<a class="button" >Demo</a> 
				<a class="button">Thesis</a> -->

				</span>
			</div>
		<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>
	</div>


	<div class="box alt">
		<div class="row gtr-50 gtr-uniform">
			<div class="col-5"><span class="image fit"><img src="images/georgios.png" alt="" /></span></div>
			<div class="col-7"><span class="image fit"><ul>
					<li style="text-align: justify; color: rgb(0, 0, 0);">Georgios Tziafas (Feb.2021 ~ )</li>
					<li style="text-align: justify;">Sim2Real Transfer of Visiolinguistic Representations for Human-Robot Interaction</li>					
				</ul>
				<a class="button"  href="https://arxiv.org/abs/2103.09720" target="_blank">Paper</a> 
				<a class="button"  href="https://youtu.be/kgQgaghf71o" target="_blank">Demo</a> 
				<!-- <a class="button">Thesis</a> -->

				</span>
			</div>
		<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>
	</div>




	<!-- ***************************************************** -->

	
	<div class="box alt">
		<div class="row gtr-50 gtr-uniform">
			<div class="col-5"><span class="image fit"><img src="images/tomasso.png" alt="" /></span></div>
			<div class="col-7"><span class="image fit">
				<ul>
					<li style="text-align: justify; color: rgb(0, 0, 0);">Tommaso Parisotto (July.2020 ~ March 2021)</li>
					<li style="text-align: justify;">MORE: Simultaneous Multi-View 3D Object Recognition and Grasp Pose Estimation</li>					
				</ul>
				<a class="button" href="https://arxiv.org/abs/2103.09863" target="_blank" >Paper</a> 
				<a class="button" href="https://github.com/tparisotto/more_mvcnn" target="_blank" >Code</a> 
				<a class="button" href="https://fse.studenttheses.ub.rug.nl/24092/" target="_blank">Thesis</a>

				</span>
			</div>
		<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>
	</div>
	<!-- ***************************************************** -->
	
	<div class="box alt">
		<div class="row gtr-50 gtr-uniform">
			<div class="col-5"><span class="image fit"><img src="images/thijs.png" alt="" /></span></div>
			<div class="col-7"><span class="image fit"><ul>
					<li style="text-align: justify; color: rgb(0, 0, 0);">Thijs Eker (Nov.2020 ~)</li>
					<li style="text-align: justify;">Viewpoint-invariant Ship Classification using 3D Reconstruction Models</li>					
					
				</ul>
				<!-- <a class="button" >Paper</a> 
				<a class="button" >Demo</a> 
				<a class="button">Thesis</a> -->

				</span>
			</div>
		<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>
	</div>

	


	<div class="box alt">
		<div class="row gtr-50 gtr-uniform">
			<div class="col-5"><span class="image fit"><img src="images/sudhakaran.jpg" alt="" /></span></div>
			<div class="col-7"><span class="image fit"><ul>
					<li style="text-align: justify; color: rgb(0, 0, 0);">Sudhakaran Jain (March.2020 ~ Dec.2020)</li>
					<li style="text-align: justify;">Open-Ended 3D Object Recognition using Dynamically Evolving Neural Networks</li>
					<!-- <a class="icon fa-file-pdf-o" href="https://www.ai.rug.nl/oel/papers/extending_GG-CNN_OEL.pdf" target="_blank"> RSS-WS</a> -->
					
				</ul>
				<a class="button" href="https://arxiv.org/pdf/2009.07213.pdf" target="_blank" >Paper</a> 
				<a class="button" href="https://youtu.be/tf4trRMyQ0Y" target="_blank">Demo</a> 
				<a class="button" href="https://github.com/sudhakaranjain/3D_DEN" target="_blank" >Code</a> 
				<a class="button" href="https://fse.studenttheses.ub.rug.nl/23621/" target="_blank">Thesis</a>

			</span></div>
			<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>
		</div>
		<!-- ***************************************************** -->
	
	<div class="box alt">
		<div class="row gtr-50 gtr-uniform">
			<div class="col-5"><span class="image fit"><img src="images/subilal.jpg" alt="" /></span></div>
			<div class="col-7"><span class="image fit"><ul>
					<li style="text-align: justify; color: rgb(0, 0, 0);">Subilal Vattimunda Purayil (April.2020 ~ Dec.2020)</li>
					<li style="text-align: justify;">Learning Deep Spatio-Temporal Features for Human Activity Classification</li>
					<!-- <a class="icon fa-file-pdf-o" href="https://www.ai.rug.nl/oel/papers/extending_GG-CNN_OEL.pdf" target="_blank"> RSS-WS</a> -->
					
				</ul>
				<!-- <a class="button" href="">Paper</a>  -->
				<!-- <a class="button" >Demo</a>  -->
				<a class="button" href="https://fse.studenttheses.ub.rug.nl/23688/" target="_blank">Thesis</a>

				</span>
			</div>
		<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>
	</div>
	<!-- ***************************************************** -->

		<div class="row gtr-50 gtr-uniform">
			<div class="col-5"><span class="image fit"><img src="images/diego.jpg" alt="" /></span></div>
			<div class="col-7"><span class="image fit"><ul>
					<li style="text-align: justify; color: rgb(0, 0, 0);">Diego Cabo Golvano (Feb.2020 ~ Dec.2020)</li>
					<li style="text-align: justify;">Exploring Novel Hierarchical Reinforcement	Learning Approaches to Lifelong Learning</li>
					<!-- <a class="icon fa-file-pdf-o" href="https://www.ai.rug.nl/oel/papers/extending_GG-CNN_OEL.pdf" target="_blank"> RSS-WS</a> -->
					
				</ul>
				<!-- <a class="button" >Paper</a>  -->
				<!-- <a class="button"  href="" target="_blank" >Demo</a>  -->
				<a class="button" href="https://fse.studenttheses.ub.rug.nl/23666/" target="_blank">Thesis</a>

			</span></div>
			<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>
		</div>
		<!-- ***************************************************** -->

		
		<div class="row gtr-50 gtr-uniform">
				<div class="col-5"><span class="image fit"><img src="images/yikun.jpg" alt="" /></span></div>
				<div class="col-7"><span class="image fit"><ul>
						<li style="text-align: justify; color: rgb(0, 0, 0);">Yikun Li (Jan.2019 ~ Aug.2019)</li>				
						<li style="text-align: justify;">Learning to Detect Grasp Affordances of 3D Objects using Deep Convolutional Neural Networks</li>
						<!-- <a class="icon fa-file-pdf-o" href="https://lcas.lincoln.ac.uk/wp/wp-content/uploads/2019/07/Yikun_RSS_TIGII.pdf" target="_blank"> TIG2-WS</a> ---
						<a class="icon style3 major fa-camera-retro" href="https://youtu.be/5_yAJCc8owo" target="_blank"> Demo</a> -->
					</ul></span>
					<a class="button" href="https://lcas.lincoln.ac.uk/wp/wp-content/uploads/2019/07/Yikun_RSS_TIGII.pdf" target="_blank">Paper</a> 
					<a class="button" href="https://youtu.be/5_yAJCc8owo" target="_blank">Demo</a> 
					<a class="button" href="https://github.com/yikun-li/pc-3d-grasp-ds" target="_blank">Dataset</a> 
					<a class="button" href="http://fse.studenttheses.ub.rug.nl/20874/" target="_blank">Thesis</a> 

				</div>
				<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>
		</div>
		<!-- ***************************************************** -->
		<div class="box alt">
				<div class="row gtr-50 gtr-uniform">
					<div class="col-5"><span class="image fit"><img src="images/mario.jpg" alt="" /></span></div>
					<div class="col-7"><span class="image fit"><ul>
							<li style="text-align: justify; color: rgb(0, 0, 0);">Mario Rios-Munoz (Jan.2019 ~ Mar. 2020)</li>
							<li style="text-align: justify;">Learning to Grasp: A Deep Learning Approach to Generalized Robust Grasp Affordance</li>
							<!-- <a class="icon fa-file-pdf-o" href="https://www.ai.rug.nl/oel/papers/extending_GG-CNN_OEL.pdf" target="_blank"> RSS-WS</a> -->
					</ul>
					<a class="button" href="https://www.ai.rug.nl/oel/papers/extending_GG-CNN_OEL.pdf" target="_blank">Paper</a> 
					<!-- <a class="button" >Demo</a>  -->
					<a class="button" href="https://fse.studenttheses.ub.rug.nl/24221/"  target="_blank">Thesis</a>
					</span>
				</div>
				<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>
			</div>
			<!-- ***************************************************** -->
	
	</div>

	<!-- ^^^^^^^^^^^^^^^^^^^^^^^^^^ BSc ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ -->
	<!-- ^^^^^^^^^^^^^^^^^^^^^^^^^^ BSc ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ -->
	<!-- ^^^^^^^^^^^^^^^^^^^^^^^^^^ BSc ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ -->
	<h4>Undergraduate Students</h3>
	<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>

		<div class="box alt">
			<div class="row gtr-50 gtr-uniform">
				<div class="col-5"><span class="image fit"><img src="images/jeroen.png" alt="" /></span></div>
				<div class="col-7"><span class="image fit"><ul>
						<li style="text-align: justify; color: rgb(0, 0, 0);">Jeroen Oude Vrielink (Feb.2021 ~  ) </li>					
						<li style="text-align: justify;">Learning grasp affordances of 3D objects using Deep Convolutional Neural Networks</li>
					</ul>
					<a class="button" href="documents/BSc_proposals/jeroen.pdf" target="_blank">Proposal</a> 
					<a class="button" href=""  target="_blank">Thesis Page</a> 

				</span></div>
			<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>
		</div>


		
		<div class="box alt">
			<div class="row gtr-50 gtr-uniform">
				<div class="col-5"><span class="image fit"><img src="images/anne.png" alt="" /></span></div>
				<div class="col-7"><span class="image fit"><ul>
						<li style="text-align: justify; color: rgb(0, 0, 0);">Anne-Jan Mein (Feb.2021 ~  ) </li>					
						<li style="text-align: justify;">Investigating the influences of different colour spaces in open-ended 3D recognition</li>
					</ul>
					<a class="button" href="documents/BSc_proposals/anne_jan.pdf" target="_blank">Proposal</a> 
					<a class="button" href=""  target="_blank">Thesis Page</a> 

				</span></div>
			<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>
		</div>

		
		<div class="box alt">
			<div class="row gtr-50 gtr-uniform">
				<div class="col-5"><span class="image fit"><img src="images/jo.png" alt="" /></span></div>
				<div class="col-7"><span class="image fit"><ul>
						<li style="text-align: justify; color: rgb(0, 0, 0);">Junhyung Jo (Feb.2021 ~  ) </li>					
						<li style="text-align: justify;">Fine-grained 3D object recognition: an approach and experiments</li>
					</ul>
					<a class="button" href="documents/BSc_proposals/junhyung.pdf" target="_blank">Proposal</a> 
					<a class="button" href=""  target="_blank">Thesis Page</a> 

				</span></div>
			<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>
		</div>
		<!-- ***************************************************** -->


		<div class="box alt">
			<div class="row gtr-50 gtr-uniform">
				<div class="col-5"><span class="image fit"><img src="images/jim.png" alt="" /></span></div>
				<div class="col-7"><span class="image fit"><ul>
						<li style="text-align: justify; color: rgb(0, 0, 0);">Jim Wu (Sep.2020 ~ Jan.2021 ) </li>					
						<li style="text-align: justify;"> Lifelong 3D Object Recognition: a comparison of deep features and handcrafted descriptors</li>
					</ul>
					<a class="button" href="documents/BSc_proposals/junhyung.pdf" target="_blank">Proposal</a> 
					<a class="button" href=""  target="_blank">Thesis Page</a> 

				</span></div>
			<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>
		</div>
		<!-- ***************************************************** -->

		<div class="box alt">
			<div class="row gtr-50 gtr-uniform">
				<div class="col-5"><span class="image fit"><img src="images/andreea.png" alt="" /></span></div>
				<div class="col-7"><span class="image fit"><ul>
						<li style="text-align: justify; color: rgb(0, 0, 0);">Andreea Toca (Feb.2020 ~ July 2020) </li>					
						<li style="text-align: justify;"> Investigating the Importance of Textures, Color Spaces and Similarity Measures in Open-Ended 3D Object Recognition</li>
					</ul>
					<a class="button" href="documents/BSc_proposals/andreea.pdf" target="_blank">Proposal</a> 
					<a class="button" href="http://fse.studenttheses.ub.rug.nl/23079/"  target="_blank">Thesis Page</a> 

				</span></div>
				<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>
			</div>
		<!-- ***************************************************** -->

			<div class="row gtr-50 gtr-uniform">
				<div class="col-5"><span class="image fit"><img src="images/Vlad.jpg" alt="" /></span></div>
					<div class="col-7"><span class="image fit"><ul>
							<li style="text-align: justify; color: rgb(0, 0, 0);">Vlad Iftime (Feb.2020 ~ July 2020) </li>					
							<li style="text-align: justify;">Autoencoder-based Representation Learning for 3D Object Recognition in Open-Ended Domains</li>
						</ul>
						<a class="button" href="documents/BSc_proposals/vlad.pdf" target="_blank">Proposal</a> 
						<a class="button" href="http://fse.studenttheses.ub.rug.nl/23078/"  target="_blank">Thesis Page</a> 
					
					</span></div>
				<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>
			</div>
		<!-- ***************************************************** -->

			<div class="row gtr-50 gtr-uniform">
					<div class="col-5"><span class="image fit"><img src="images/roberto.png" alt="" /></span></div>
						<div class="col-7"><span class="image fit"><ul>
							<li style="text-align: justify; color: rgb(0, 0, 0);">Roberto Navarro (Feb.2020 ~ July 2020) </li>					
							<li style="text-align: justify;">Learning to Grasp 3D objects using Deep Convolutional Neural Network</li>
						</ul>
						<a class="button" href="documents/BSc_proposals/roberto.pdf" target="_blank">Proposal</a> 
						<a class="button" href="http://fse.studenttheses.ub.rug.nl/23009/"  target="_blank">Thesis Page</a> 

					</span></div>
					<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>
			</div>
		<!-- ***************************************************** -->

			<div class="row gtr-50 gtr-uniform">
				<div class="col-5"><span class="image fit"><img src="images/nils.jpg" alt="" /></span></div>
					<div class="col-7"><span class="image fit"><ul>
						<li style="text-align: justify; color: rgb(0, 0, 0);">Nils Keunecke (Feb.2020 ~ July 2020) </li>					
						<li style="text-align: justify;">Three-Dimensional Object Recognition using OrthographicNet and Color Constancy	</li>
					</ul>
					<a class="button" href="documents/BSc_proposals/nils.pdf" target="_blank">Proposal</a> 
					<a class="button" href="https://fse.studenttheses.ub.rug.nl/23372/" target="_blank">Thesis Page</a> 
					<a class="button" href="https://youtu.be/IdZPspWsqc0" target="_blank">Demo</a> 
					
				</span></div>
				<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>
			</div>
		<!-- ***************************************************** -->
				
			<div class="row gtr-50 gtr-uniform">
				<div class="col-5"><span class="image fit"><img src="images/sandra.jpg" alt="" /></span></div>
				<div class="col-7"><span class="image fit"><ul>
						<li style="text-align: justify; color: rgb(0, 0, 0);">Sandra Bedrossian (Jan.2019 ~ July 2019) </li>					
						<li style="text-align: justify;">Vehicle License Plate Recognition Using Pixel Information</li>
					</ul>
					<a class="button" href="documents/BSc_proposals/sandra.pdf" target="_blank">Proposal</a> 
					<a class="button" href="http://fse.studenttheses.ub.rug.nl/20437/" target="_blank">Thesis Page</a> 

				</span></div>
				<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>
			</div>
		<!-- ***************************************************** -->
	
			<!-- <div class="row gtr-50 gtr-uniform">
					<div class="col-5"><span class="image fit"><img src="images/plamen.jpg" alt="" /></span></div>
					<div class="col-7"><span class="image fit"><ul>
							<li style="text-align: justify; color: rgb(0, 0, 0);">Plamen Dragiyski (Jan.2019 ~)</li>
							<li style="text-align: justify;">Next-Best-View Prediction for Mobile Robots: Should Robots Look Further to Recognize Better?</li>
						</ul></span>
						<a class="button">Proposal</a> 
						<a class="button">Thesis Page</a>
					</div>
					<div class="col-4"><span class="image fit"><img src="" alt="" /></span></div>
			</div>
		</div>
	 -->

</section>

<!-- ^^^^^^^^^^^^^^^^^^^^^^^^^^ Section 5 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ -->
<!-- ^^^^^^^^^^^^^^^^^^^^^^^^^^ Section 5 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ -->
<!-- ^^^^^^^^^^^^^^^^^^^^^^^^^^ Section 5 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ -->

<section id="teaching">
<div class="container">
<h3 style="color: rgb(0, 0, 0);">Teaching</h3>

<p style="text-align: justify; color: rgb(0, 0, 0);">
Academic year 2020/2021: <br/>
<a class="icon fa-book" style="color: rgb(0, 0, 255);" href="https://rugcognitiverobotics.github.io/" target="_blank"> WMAI003-05: Cognitive Robotics (Coordinator)</a><br/>
<a class="icon fa-book" style="color: rgb(0, 0, 255);" href="https://www.rug.nl/ocasys/let/vak/show?code=KIM.ML09" target="_blank"> WMAI18002: Deep Learning (Lecturer)</a><br/>
<a class="icon fa-book" style="color: rgb(0, 0, 255);" href="https://www.rug.nl/ocasys/frw/vak/show?code=KIB.AS03" target="_blank"> KIB.AS03: Autonomous Systems (Lecturer)</a><br/>
<!-- <a class="icon fa-book" style="color: rgb(0, 0, 255);" href="https://www.rug.nl/ocasys/rug//vak/show?code=WMAI18002" target="_blank"> WMAI18002: Deep Learning (Assistant)</a><br/> -->
	
<p style="text-align: justify; color: rgb(0, 0, 0);">
Academic year 2019/2020: <br/>
<a class="icon fa-book" style="color: rgb(0, 0, 255);" href="https://rugcognitiverobotics.github.io/" target="_blank"> KIM.CROB04: Cognitive Robotics (Coordinator)</a><br/>
<a class="icon fa-book" style="color: rgb(0, 0, 255);" href="https://www.rug.nl/ocasys/frw/vak/show?code=KIB.AS03" target="_blank"> KIB.AS03: Autonomous Systems (Lecturer)</a><br/>
<a class="icon fa-book" style="color: rgb(0, 0, 255);" href="https://www.rug.nl/ocasys/let/vak/show?code=KIM.ML09" target="_blank"> KIM.ML09: Machine Learning (Assistant)</a><br/>
<!-- <a class="icon fa-book" style="color: rgb(0, 0, 255);" href="https://www.rug.nl/ocasys/rug//vak/show?code=WMAI18002" target="_blank"> WMAI18002: Deep Learning (Assistant)</a><br/> -->

<p style="text-align: justify; color: rgb(0, 0, 0);">
Academic year 2018/2019: <br/>
<a class="icon fa-book" style="color: rgb(0, 0, 255);" href="https://www.rug.nl/ocasys/frw/vak/show?code=KIB.AS03" target="_blank"> KIB.AS03: Autonomous Systems (Lecturer)</a><br/>
<a class="icon fa-book" style="color: rgb(0, 0, 255);" href="https://www.rug.nl/ocasys/let/vak/show?code=KIM.ML09" target="_blank"> KIM.ML09: Machine Learning (Assistant)</a><br/>
<a class="icon fa-book" style="color: rgb(0, 0, 255);" href="https://www.rug.nl/ocasys/rug//vak/show?code=WMAI18002" target="_blank"> WMAI18002: Deep Learning (Assistant)</a><br/>


</p>
</div>
</section>

<!-- ^^^^^^^^^^^^^^^^^^^^^^^^^^ Section 6 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ -->
<!-- ^^^^^^^^^^^^^^^^^^^^^^^^^^ Section 6 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ -->
<!-- ^^^^^^^^^^^^^^^^^^^^^^^^^^ Section 6 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ -->
<section id="positions">
	<div class="container">
	<h3 style="color: rgb(0, 0, 0);"> Open Positions</h3>
	
	<article>
	<p style="text-align: justify; color: rgb(0, 0, 0);">
			If you are interested in doing your Bachelor/Master/PhD thesis in one of the above areas, or working on a project with me, please send me an e-mail including:
			</p>
			
			<ul class="feature-icons" style="text-align: justify;">
				<li class="fa-book" style="color: rgb(0, 0, 255);">Short CV</li>
				<li class="fa-cubes" style="color: rgb(0, 0, 255);">Short motivation letter</li>
			</ul>
			
			
			<p style="text-align: justify; color: rgb(0, 0, 0);">
			The motivation letter should state (½ - 1 page):
			<ul class="feature-icons">
				<li class="fa-diamond" style="color: rgb(0, 0, 255);">Topics that you are interested in</li>
				<li class="fa-signal" style="color: rgb(0, 0, 255);">Type of project (theoretical/applied)</li>
				<li class="fa-calendar" style="color: rgb(0, 0, 255);">Intended starting date</li>
				<li class="fa-folder-open-o" style="color: rgb(0, 0, 255);">Your relevant experiences</li>
				<!-- <li class="fa-code" style="color: rgb(0, 0, 255);">Programming languages and related</li> -->
			</ul>
			</p>
		</article>
	</div>
</section>
<section>
<div class="container">
<h3 style="color: rgb(0, 0, 0);">Contact</h3>

	<div class="row">
		<div class="col-6 col-12-small">
				<p style="text-align: justify; color: rgb(0, 0, 0);">
					<br/><br/>
					Dr. Hamidreza Kasaei<br/>
					Artificial Intelligence Department,<br/>
					<a style="color: rgb(0, 0, 0);" href="https://www.rug.nl/" target="_blank"> University of Groningen</a>, <br/>
					Bernoulliborg building,<br/>
					Nijenborgh 9 9747 AG Groningen, <br/>
					The Netherlands. <br/>
					Office: 340 <br/>
					Tel: +31-50-363-33926<br/>
					E-mail: hamidreza.kasaei@rug.nl<br/>																							
				</p>
			</div>
			<div class="col-6 col-12-small">
				<div class="one-half" align="right" style="padding-left:20px">

					<!-- iframe plugin v.4.5 wordpress.org/plugins/iframe/ -->
					<iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d2387.742488603945!2d6.534234515834465!3d53.2403922799569!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x47c9cd1a4fa87a5d%3A0xdd27b1b9723bb97b!2sBernoulliborg%2C%209747%20AG%20Groningen!5e0!3m2!1sen!2snl!4v1618743298198!5m2!1sen!2snl" width="600" height="400" style="border:0;" allowfullscreen="" loading="lazy"></iframe>				
				</div>
			</div>
	</div>


	<!-- <p style="text-align: justify; color: rgb(0, 0, 0);">

	Dr. Hamidreza Kasaei<br/>
	Artificial Intelligence Department,<br/>
	<a style="color: rgb(0, 0, 0);" href="https://www.rug.nl/" target="_blank"> University of Groningen</a>, <br/>
	Bernoulliborg building,<br/>
	Nijenborgh 9 9747 AG Groningen, <br/>
	The Netherlands. <br/>
	Office: 340 <br/>
	Tel: +31-50-363-33926<br/>
	E-mail: hamidreza.kasaei@rug.nl<br/>

	</p> -->
</div>
</section>


<!-- Footer -->
<section id="footer">
	<div class="container">
		<ul class="copyright">
			<!-- <li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li> -->
		</ul>
	</div>
</section>

</div>

<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/jquery.scrollex.min.js"></script>
	<script src="assets/js/jquery.scrolly.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>

</body>
</html>
